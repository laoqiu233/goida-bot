# GOIDA-BOT
## News aggregator with GPT

Project for "Wireless networks" discipline.

## TODOs
- [x] Скачиваем новости из нескольких заданных RSS источников
- [x] Конвертируем скачанные html в pdf
- [x] Передаем pdf на вход LLM (GPT4All)
- [x] Задать промпт, например "Какие сегодня есть события (новости) упоминающие о ФИО" (спортсмен, политик, киноактер и т.п.)
Или просим "Выведи полный текст новостей о ФИО" (чтобы не пересказ а оригинальный текст новости был)
- [x] Автоматизировать предыдущие пункты
- [ ] Индексировать и использовать в поисках full_text и summary вместо изначальной PDF
- [x] Занести векторное хранилище в постгрес
- [X] Название статьей хранить в БД
- [ ] Время публикации протянуть в БД
- [ ] Подумать про подписки на поиски
- [ ] Улучшить результат с помощью различных моделей (Mistral, Deepseek, Snoozy), подпираем формулировку запроса новостей
- [ ] Доработать документацию по запуску

## Запуск проекта

### Скачать Ollama

Базовые компоненты для загрузки новостей и индексации запускается через docker-compose,
а модели запускаются через [Ollama](https://ollama.com/), которую необходимо установить на системе хоста, поскольку Ollama в контейнере достаточно нестабильна, ее необходимо будет установить вручную на системе.

### Скачать доступные модельки на сайте Ollama

В [библиотеке Ollama](https://ollama.com/library) доступно 
множество готовых моделей. После выбора можно запустить команду `ollama pull <название модели>` для скачивания. 

### Выбрать модельки кастомные

После установки ollama необходимо подобрать языковую модель и модель эмбеддингов в формате [GGUF](https://huggingface.co/models?library=gguf&sort=trending). При тестировании были выбраны модели:
- Языковая модель - [llama-3](https://huggingface.co/ruslandev/llama-3-8b-gpt-4o-ru1.0-gguf/tree/main) (выбран вариант Q8_0)
- Эмбеддинговая модель - [multilingual-e5-large](https://huggingface.co/KeyurRamoliya/multilingual-e5-large-GGUF/tree/main)

После того, как модельки будут скачаны, необходимо их
переместить в одну папку и создать для каждого `Modelfile`

Пример директории:
```
- ggml-model-Q8_0.gguf.gguf (Языковая модель)
- llm.Modefile
- multilingual-e5-large-q8_0.gguf (Эмбеддинговая модель)
- embedding.Modefile
```

Содержание `llm.Modelfile`:
```
FROM multilingual-e5-large-q8_0.gguf
```

То есть в каждом  `Modelfile` нужно прописать лишь путь до 
файла `.gguf`

После чего в терминале необходимо прописать команды, чтобы создать модели в реестре 
ollama под заданным названием
```
ollama create -f llm.Modelfile my-model
ollama create -f embedding.Modelfile my-embed 
```

### Проверить, что модели доступны

Прописать команду `ollama ls`, можем получить следующий вывод:

```
NAME                       ID              SIZE      MODIFIED    
my-embed:latest            cf3e52d7e16f    603 MB    3 days ago     
my-model:latest            712de27f7a6b    8.5 GB    3 days ago     
nomic-embed-text:latest    0a109f422b47    274 MB    10 days ago    
gemma2:latest              ff02c3702f32    5.4 GB    10 days ago
```

После чего можно запустить в терминиле модель через команду 
`ollama run <название модели>`:

### Настроить проект

Необходимо открыть файл `docker-compose.yml` и изменить следующие строчки:
```
PGPT_OLLAMA_LLM_MODEL: my-model
PGPT_OLLAMA_EMBEDDING_MODEL: my-embed
```

Нужно указать те модели, которые были получены на предыдущих этапах.

### Запустить

Перед запуском лучше создать папку `content` в корне проекта, где будут хранится PDF версии статьи. 

После чего следует запустить ollama и ввести следующую команду:
```
docker compose up -d
```

Затем нужно задать фиды, что можно сделать в файле make_feeds.sql, затем выполнить команду:
```
make feeds
```

При повторном выполнении команды make feed все затирается! И также необходимо выключить все кроме БД:
```
docker compose down # Все выключаем
docker compose up -d postgres # Запускаем только БД
make feeds
docker compose up -d
```

После запуска можно наблюдать за состоянием индексации статьей, увидеть соотношение проиндексированных/скаченных через команду:

```
make indexed
```

Чтобы посмотреть логи сервисов можно использовать команду:
```
docker compose logs -f
```

А повзаимодействовать с ЛЛМ можно по ссылке `http://localhost:8001`
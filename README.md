# GOIDA-BOT
## News aggregator with GPT

Project for "Wireless networks" discipline.

## TODOs
- [x] Скачиваем новости из нескольких заданных RSS источников
- [x] Конвертируем скачанные html в pdf
- [x] Передаем pdf на вход LLM (GPT4All)
- [x] Задать промпт, например "Какие сегодня есть события (новости) упоминающие о ФИО" (спортсмен, политик, киноактер и т.п.)
Или просим "Выведи полный текст новостей о ФИО" (чтобы не пересказ а оригинальный текст новости был)
- [x] Автоматизировать предыдущие пункты
- [X] Индексировать и использовать в поисках full_text и summary вместо изначальной PDF
- [x] Занести векторное хранилище в постгрес
- [X] Название статьей хранить в БД
- [X] Время публикации протянуть в БД
- [X] Подумать про подписки на поиски
- [ ] Улучшить результат с помощью различных моделей (Mistral, Deepseek, Snoozy), подпираем формулировку запроса новостей
- [X] Доработать документацию по запуску

## Запуск проекта

### Скачать Ollama

Базовые компоненты для загрузки новостей и индексации запускается через docker-compose,
а модели запускаются через [Ollama](https://ollama.com/), которую необходимо установить на системе хоста, поскольку Ollama в контейнере достаточно нестабильна, ее необходимо будет установить вручную на системе.

Чтобы запустить Ollama, необходимо ввести команду:

```
OLLAMA_HOST=0.0.0.0 ollama serve
```

Затем зайти на сайт http://localhost:11434/ и убедится, что ответом будет строчка:

```
Ollama is running
```

### Скачать доступные модельки на сайте Ollama

В [библиотеке Ollama](https://ollama.com/library) доступно 
множество готовых моделей. После выбора можно запустить команду `ollama pull <название модели>` для скачивания. 

### Выбрать модельки кастомные

После установки ollama необходимо подобрать языковую модель и модель эмбеддингов в формате [GGUF](https://huggingface.co/models?library=gguf&sort=trending). При тестировании были выбраны модели:
- Языковая модель - [llama-3](https://huggingface.co/ruslandev/llama-3-8b-gpt-4o-ru1.0-gguf/tree/main) (выбран вариант Q8_0)
- Эмбеддинговая модель - [multilingual-e5-large](https://huggingface.co/KeyurRamoliya/multilingual-e5-large-GGUF/tree/main)

После того, как модельки будут скачаны, необходимо их
переместить в одну папку и создать для каждого `Modelfile`

Пример директории:
```
- ggml-model-Q8_0.gguf.gguf (Языковая модель)
- llm.Modefile
- multilingual-e5-large-q8_0.gguf (Эмбеддинговая модель)
- embedding.Modefile
```

Содержание `llm.Modelfile`:
```
FROM multilingual-e5-large-q8_0.gguf
```

То есть в каждом  `Modelfile` нужно прописать лишь путь до 
файла `.gguf`

После чего в терминале необходимо прописать команды, чтобы создать модели в реестре 
ollama под заданным названием
```
ollama create -f llm.Modelfile my-model
ollama create -f embedding.Modelfile my-embed 
```

### Проверить, что модели доступны

Прописать команду `ollama ls`, можем получить следующий вывод:

```
NAME                       ID              SIZE      MODIFIED    
my-embed:latest            cf3e52d7e16f    603 MB    3 days ago     
my-model:latest            712de27f7a6b    8.5 GB    3 days ago     
nomic-embed-text:latest    0a109f422b47    274 MB    10 days ago    
gemma2:latest              ff02c3702f32    5.4 GB    10 days ago
```

После чего можно запустить в терминиле модель через команду 
`ollama run <название модели>`:

### Настроить проект

Необходимо открыть файл `docker-compose.yml` и изменить следующие строчки:
```
PGPT_OLLAMA_LLM_MODEL: my-model
PGPT_OLLAMA_EMBEDDING_MODEL: my-embed
```

Нужно указать те модели, которые были получены на предыдущих этапах. Если вы загрузили модели не меняя названия (остались 
my-model и my-embed), то ничего не нужно менять в файле.

### Запустить

Перед запуском лучше создать папку `content` в корне проекта, где будут хранится PDF версии статьи. 

После чего следует запустить ollama и ввести следующую команду:
```
docker compose up -d migrate
```

Затем нужно задать фиды, что можно сделать в файле make_feeds.sql, затем выполнить команду:
```
make feeds
```

Таким образом мы подготовили базу данных для статьей, теперь запустим остальные сервисы:

```
docker compose up -d
```

*Можно убрать флаг -d, если нужно посмотреть на логи*

При повторном выполнении команды make feed все затирается! И также необходимо выключить все кроме БД:
```
docker compose down -v # Все выключаем
docker compose up -d migrate # Запускаем только БД
make feeds
docker compose up -d
```

После запуска можно наблюдать за состоянием индексации статьей, увидеть соотношение проиндексированных/скаченных через команду:

```
make indexed
```

Чтобы посмотреть логи сервисов можно использовать команду:
```
docker compose logs -f
```

А повзаимодействовать с ЛЛМ можно по ссылке `http://localhost:8001`

### Режим периодических вопросов

Чтобы по фиксированному запросу получить суммаризацию статьей с исходниками, можно воспользоваться командой `make ask`. Которая запустит программу, 
запрашивающую раз в пол часа ответ.

Пример:
```
% make ask

poetry run python -m asker

Введите ваш запрос (будет выполняться раз в 30 минут):Новый год

Нашлось 92 статьей за последние 24 часа

1. Для подготовки к предновогоднему периоду важно следить за физическим и психологическим состоянием.

2. Женщины должны спать не менее двух часов больше мужчин из-за разницы в их физиологии, рекомендуются 9-10 часов сна для женщин и 7-8 часов - для мужчин.

3. Новогодняя корзина включает в себя различные продукты: горячее блюдо, овощная нарезка, закуски, бутерброды с красной икрой (самые дорогие), салаты, фрукты, торт, игристое вино, столовое вино, коньяк, минералку, соки и чай.

4. Цены на продукты выросли: овощная нарезка (+12%), торт (+12%), бутерброды с красной икрой (+32% по сравнению с предыдущим годом), шампанское (+10%). Картофель, икра и сливочное масло также подорожали.

5. Курица упала в цене на 7%.
Исходники:
1. Россиянам дали советы, как мобилизовать организм в предновогодний период [Новости Mail.ru] - https://news.mail.ru/society/63876855/
2. Названа сумма расходов россиян на новогодний стол [Лента.ру] - https://lenta.ru/news/2024/12/01/happy-new-year/
```

Промпт/частоту запросов можно самостоятельно изменить в скрипте `asker/__main__.py`